# cs224n Lecture 13

부제 : Machine Learning이라는 새로운 task와 sequence-to-sequence 라는 새로운 __architecture__

## 개요

* Unknown words
* TagLM, CoVe
* ELMo
* SNLI, SRL, Coref, NER, SST-5
* ULMfit, transfer learning
* Transformer
* Multi-head attention
* Layer normalization
* Positional encoding, segment embedding
* BERT, GPT
* BookCorpus, GLUE

## Unknown words
지금까지는 one representation of words 만 있다고 가정했다(Word2vec, Glove, fastText).  
Pre-Trained word vectors 를 사용하는 것이 1~2%정도 성능을 개선하는 것으로 나타났다. Pre-Trained word vectors 를 가지고 추가적으로 Unknown 단어들을 un supervised learning으로  학습하면 되는 것이다.

또는 그냥 랜덤 벡터를 넣기도 한다. 각 단어는 꼭 하나의 벡터와 매핑이 되도록 해주는 것이다.

::: tip Tip from Manning regarding unknown words.
Train Time : Vocab is {words occurring, say, >= 5times} + {<UNK>}
             Map all rarer words (<5) to <UNK>, train a word vector for it.
Runtime : use <UNK> when out-of-vocabulary(OOV) words occur
:::
그런데 이렇게 하면 서로 다른 단어였던 UNK words 들을 구분할 수 없다는 문제가 있다(identity나 meaning 차원에서).  
그래서 char-level models 를 이용해서 vector를 만드는 것이 좋다!

Unknown vocab이 나왔을 때, 인코더에서 봤던 단어를 가져오는 경우도 있다.

어쨌거나 이 방법은 word-sense를 정확히 반영하지 못한다. Context를 반영하지 못한다는 것(star- universe, hollywood senses).

## TagLM, CoVe
그런데 LSTM 모델을 생각해보면 context-specific word representations를 매 position 마다 잘 산출하는 것을 확인할 수 있다.
![](/TIL/ML/13_LSTM.png)

일단 word에 context를 가미하고 싶고, standardly learn task RNN only on small task-labeled data 라는 한계를 극복하고자 하는 시도다.  
이를 위해서 NLM을 unlabeled corpus 에 학습시키는 semi-supervised approach를 해보자는 것.

![](/TIL/ML/13_TagLM1.png)
![](/TIL/ML/13_TagLM2.png)

아무튼 Peters et al.(2017) 논문이 밝혀낸 것은 1) bidirectional LM이 only forward 보다 도움이 된다는 것과, 2) supervised data를 학습한 LM은 도움이 되지 않는다는 것, 3) 큰 LM 을 가지는게 적은 모델에 대해 도움이 된다(Q. __ppl 30 이 무슨 의미지?__ ).

Cove 모델은 매닝 성님이 그냥 skip 해버렸다.

## ELMo : Embeddings from Language Models
Peters etal.(2018)
TagLM의 업글 버전, Bi-NLM을 학습하고 그 모든 Layer을 prediction에 활용해라.  
별로 크지 않은 LM을 활용하고자 한다. character CNN을 활용해서 initial word representation 을 뽑아내는데 이를 통해 parameter의 개수를 줄인다.  
__residual connection__ 을 사용한다.
2개의 biLSTM Layer을 활용
Input 과 Output(softmax) token의 parameter를 묶고 (parameter를 묶는 다는게 뭐지?? concat 한다는 것인가?) 이렇게 묶인 것을 forward와 backward LM 사이에 묶는다.

## SNLI, SRL, Coref, NER, SST-5
Stanfor Natural Language Inference : 추론(contradiction, neutral, entailment)
Semantic Role Labeling : 
나머지는 대충 무엇인지 다 알고 있는 것들
+ CoLa : corpus of Linguistics Acceptability(Warstadt et al., 2018) consists of English acceptability judgements frawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence.


## ULMfit, transfer learning
Universal Language Model Fine-tuning
LM 에서 학습 정보를 활용하려고 한다는 점에서는 ELMO와 동일하다.
LM 에서 학습된 네트워크를 그대로 활용한다.

![](/TIL/ML/13_ULMdfit.png)

## Transformer
### Basic blocks
> Dot-product Attention
> Scaled Dot-product Attention
> Self-attention at the encoder

### Multi-head attention

### Encoder
### Decoder
### Tips and Tricks of Tranformer
## Layer normalization

## Positional encoding, segment embedding

## BERT, GPT
BERT : Bidirectional Encoder Representations from Transformers

__"Words can see themselves"__ : 

## BookCorpus, GLUE


