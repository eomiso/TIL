<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>CSS224n Lecture 12 | Eomiso&#39;s TIL(Today I Learned)</title>
    <meta name="generator" content="VuePress 1.4.1">
    <link rel="icon" href="/TIL/logo.png">
    <meta name="description" content="Write what you have learned today">
    <link rel="preload" href="/TIL/assets/css/0.styles.34c10105.css" as="style"><link rel="preload" href="/TIL/assets/js/app.2722f0f6.js" as="script"><link rel="preload" href="/TIL/assets/js/2.2c4c9c51.js" as="script"><link rel="preload" href="/TIL/assets/js/22.65a2edf5.js" as="script"><link rel="prefetch" href="/TIL/assets/js/10.2f8e0e30.js"><link rel="prefetch" href="/TIL/assets/js/11.1ff1a57e.js"><link rel="prefetch" href="/TIL/assets/js/12.15c40c13.js"><link rel="prefetch" href="/TIL/assets/js/13.7bf652e1.js"><link rel="prefetch" href="/TIL/assets/js/14.c9414a20.js"><link rel="prefetch" href="/TIL/assets/js/15.2d01f370.js"><link rel="prefetch" href="/TIL/assets/js/16.0d89dffe.js"><link rel="prefetch" href="/TIL/assets/js/17.07c38e4d.js"><link rel="prefetch" href="/TIL/assets/js/18.442c0c06.js"><link rel="prefetch" href="/TIL/assets/js/19.1b5bf633.js"><link rel="prefetch" href="/TIL/assets/js/20.86bba05d.js"><link rel="prefetch" href="/TIL/assets/js/21.e544340d.js"><link rel="prefetch" href="/TIL/assets/js/23.8b174a85.js"><link rel="prefetch" href="/TIL/assets/js/24.64fc03d5.js"><link rel="prefetch" href="/TIL/assets/js/25.ce0ae723.js"><link rel="prefetch" href="/TIL/assets/js/26.8b8ee881.js"><link rel="prefetch" href="/TIL/assets/js/27.bf8c97e7.js"><link rel="prefetch" href="/TIL/assets/js/28.142e3c12.js"><link rel="prefetch" href="/TIL/assets/js/29.81bd0da5.js"><link rel="prefetch" href="/TIL/assets/js/3.313ef446.js"><link rel="prefetch" href="/TIL/assets/js/30.359dbc7f.js"><link rel="prefetch" href="/TIL/assets/js/31.0727ec46.js"><link rel="prefetch" href="/TIL/assets/js/32.fe1c9148.js"><link rel="prefetch" href="/TIL/assets/js/33.a56a58e6.js"><link rel="prefetch" href="/TIL/assets/js/34.27b6efed.js"><link rel="prefetch" href="/TIL/assets/js/35.62f50cfb.js"><link rel="prefetch" href="/TIL/assets/js/36.87dcc99f.js"><link rel="prefetch" href="/TIL/assets/js/37.0e07a91e.js"><link rel="prefetch" href="/TIL/assets/js/38.205eff79.js"><link rel="prefetch" href="/TIL/assets/js/39.aaa60523.js"><link rel="prefetch" href="/TIL/assets/js/4.b761c68e.js"><link rel="prefetch" href="/TIL/assets/js/40.6b01a59c.js"><link rel="prefetch" href="/TIL/assets/js/41.1eb27f61.js"><link rel="prefetch" href="/TIL/assets/js/42.6841e3f3.js"><link rel="prefetch" href="/TIL/assets/js/43.de455827.js"><link rel="prefetch" href="/TIL/assets/js/5.a3718665.js"><link rel="prefetch" href="/TIL/assets/js/6.13ab33a4.js"><link rel="prefetch" href="/TIL/assets/js/7.d95456ce.js"><link rel="prefetch" href="/TIL/assets/js/8.8630e26e.js"><link rel="prefetch" href="/TIL/assets/js/9.1c63a592.js">
    <link rel="stylesheet" href="/TIL/assets/css/0.styles.34c10105.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/TIL/" class="home-link router-link-active"><!----> <span class="site-name">Eomiso's TIL(Today I Learned)</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/eomiso" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://eomiso.blogspot.com/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/eomiso" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="https://eomiso.blogspot.com/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>1. Python</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>2. Git</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>3. Java</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>4. C++</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>5. Machine Learning</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/TIL/ML/cssn224_1.html" class="active sidebar-link">CSS224n Lecture 12</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#개요" class="sidebar-link">개요</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#out-of-vocabulary" class="sidebar-link">Out-of-vocabulary</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#phonemes-morphemes" class="sidebar-link">phonemes, morphemes</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#character-level-models" class="sidebar-link">Character-level models</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#byte-pair-encoding-subword-models" class="sidebar-link">Byte Pair encoding(Subword-models)</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#wordpiece-sentencepiece" class="sidebar-link">wordpiece, sentencepiece</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#hybrid-nmt" class="sidebar-link">Hybrid NMT</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#highway-network-vs-lstm" class="sidebar-link">Highway network vs. LSTM</a></li><li class="sidebar-sub-header"><a href="/TIL/ML/cssn224_1.html#fasttest" class="sidebar-link">FastTest,</a></li></ul></li><li><a href="/TIL/ML/cssn224_2.html" class="sidebar-link">CS224n 6강</a></li><li><a href="/TIL/ML/cs224n_3.html" class="sidebar-link">cs224n Lecture 8</a></li><li><a href="/TIL/ML/cs224n_4.html" class="sidebar-link">cs224n Lecture 13</a></li><li><a href="/TIL/ML/cs224n_5.html" class="sidebar-link">cs224n Lecture 14</a></li><li><a href="/TIL/ML/cs224n_6.html" class="sidebar-link">cs224n 10강 정리</a></li><li><a href="/TIL/ML/cs224n_7.html" class="sidebar-link">cs224n lecture 15</a></li><li><a href="/TIL/ML/ml_dict.html" class="sidebar-link">Machine Learning 용어 사전</a></li><li><a href="/TIL/ML/style_transfer.html" class="sidebar-link">Style Transfer from Non-Parallel Text by Cross-Alignment 리뷰</a></li><li><a href="/TIL/ML/using_bert.html" class="sidebar-link">Bert 사용기</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>6. Computer Structure</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Mics.</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="css224n-lecture-12"><a href="#css224n-lecture-12" class="header-anchor">#</a> CSS224n Lecture 12</h1> <h2 id="개요"><a href="#개요" class="header-anchor">#</a> 개요</h2> <ul><li>Out-of-vocabulary</li> <li>phonemes, morphemes</li> <li>Character-level models</li> <li>Byte Pair encoding</li> <li>wordpiece, sentencepiece</li> <li>Highway network vs. LSTM</li> <li>Hybrid NMT</li> <li>FastTest, &lt;,&gt;</li></ul> <h2 id="out-of-vocabulary"><a href="#out-of-vocabulary" class="header-anchor">#</a> Out-of-vocabulary</h2> <p>Below word level로 구현해야할 이유들에는 Rich Morphology를 가진 언어들이 있다는 사실과, transliteration, informal spelling 등을 실제 문제에서 맞닥뜨릴 가능성이 있기 때문이다.</p> <h2 id="phonemes-morphemes"><a href="#phonemes-morphemes" class="header-anchor">#</a> phonemes, morphemes</h2> <p>Human Language is discrete. While the Phonix is continuous with in a continuous space of mouth. -&gt; 이것이 바로 음운의 개념으로 이어지는 것이다.  (<strong>Categorical perception</strong>)
Morphemes are a minimal level that has meaning(-ly, -ate, -en). -&gt; 형태소 거의 아무도 이러한 요소를 NLP DeepLearning 에 적용하는 것이 일반적이지 않다. 그 이유는 character based n-gram 만적용해도 충분하기 때문이다(Wickelphones(Rumelhart &amp; McClelland 1986, Microsoft's DSSM(Huang, He, Gao, Deng, Acero, &amp; Hect 2013)).</p> <p>형태소(un- + fortun(e)- + -ate + -ly 어간, 파생접사 등등)를 같은걸 모델에 적용할 수 있는 방법엔 어떤 것이 있을까?
언어마다 Language System 이 다른데, 이것을 어떻게 해결할 것인가?(no word separation in Chinese. Even european languages didn't have space before Medieval times)</p> <h2 id="character-level-models"><a href="#character-level-models" class="header-anchor">#</a> Character-level models</h2> <p>2015년까지만 해도 word가NLP의 기본 단위였다. 그러나  Word embeddings를 character embedings 에서 시작하는 모델이 연구되기 시작하였다. 기본적으로 위의 out-of-vocabulary 상황에서도 학습을 할 수 있기 위한 방법이다. Connected Language(No distinction between words with via spaces)의 문제에 사용할 수 있다. 비슷한 철자를 가진 단어는 비슷하게 엠베딩이 될 수 있다.</p> <p>2가지 방식: 1) word embedding을 character embedding에서 시작하는 방식, 2) character embedding만 하는 방식 으로 2가지가 있는데 둘 다 좋은 결과를 내고 있는 것으로 알려졌다.
vector of 'h', vector of 'a', vector of 't' 가 충분한 neural net에 의해 파악되고 나면 'hat'의 의미를 추출하는데 도움이 될 것이라는 것.</p> <blockquote><p>A very powerful combinatory models with a lot of param in them. Helps store and buil representations of meaning from multi-letter groups, in such a way that they can model the meaning of morphemes and larger units and therefore put together word meanings.</p></blockquote> <p>Deep Learning 에선 phoneme이 사용되지 않는데, 이는 phoneme 분석이 된 거대 데이터를 구하기가 어려웠기 때문이다. 그런데 언어 중에는 굉장히 phonemic(diagraph) 한 철자시스템을 가지고 있는 경우도 있다. 중국어는 Ideographic 하고, 일본어는 syllabic 하다. <strong>Character base</strong> 로 무언가를 한다고 했을 때 언어마다 철자법이 다 다르다는 것을 알아두어야 한다(같은 tri-gram 이라도 그것이 담고 있는 정보가 다를 수 있다. e.g. tho - 나는
어쨌든__Character level model__은 뉴럴기계번역에서 사용되었는데, 처음에는 잘 안되다가, Decoder 부터 시작해서(Junyoung Chung, Kyunghyun Cho, Yoshua Bengio. arXiv 2016) 현재는 매우 기대할만한 결과를 보이고 있다.</p> <blockquote><p><em>Fully character-Level Neural Machine Translation without explicit seqmentation(Jason Lee, Kyunghyun Cho, Thomas Huffmann, 2017)</em></p></blockquote> <blockquote><p><em>Revisiting Character-Based Neural Machine Translation with Capacity and
Compression</em>(Cherry, Foster, Bapna, Firat, Macherey, Google AI. 2018)</p></blockquote> <h2 id="byte-pair-encoding-subword-models"><a href="#byte-pair-encoding-subword-models" class="header-anchor">#</a> Byte Pair encoding(Subword-models)</h2> <p>이러한 sub-word model에는 2가지 트렌드가 있는데, 하나는 word-level model과 같은 아키텍쳐를 사용하는 대신에 단위를 줄이는 것이고(using smaller units: &quot;word pieces&quot;) 다른 하나는 Hybrid 한 아키텍쳐를 사용하는 것이다.</p> <p>Byte Pair Encoding 은 원래 <strong>compression algorithm</strong> 이다. most frequent byte pair를 새로운 byte로 병합(?) 하는 방식인데, 결과적으로 ngrams 대신에 bytes를 사용하는 것이다. unicode-8만 해도 200,000 글자가 넘는데, 이 것을 bytes로 바꿔서 사용하는 것이다. 자세한 구현패턴은 슬라이드 참고.</p> <h2 id="wordpiece-sentencepiece"><a href="#wordpiece-sentencepiece" class="header-anchor">#</a> wordpiece, sentencepiece</h2> <p>BPE의 일종으로 구글에서 사용중인 알고리즘 (Google NMT).</p> <p>단순한 char n-gram count를 사용하는 게 아니라, greedy approximation을 사용해서 해당 모델의 Log likelihood을 극대화 하는 방식으로 pieces를 선택하는 방식으로 word 단위와 sentence 단위로 적용하는 2가지 방식이 있다. 후자는 전자에 언어마다 tokenizer가 필요하다는 점을 극복하고, 빈칸을 _ 라는 특수기호로 처리해서 character로 본다.
BERT 가 이러한 알고리즘을 이용하기 때문에 word pieces 를 사용하고 있음을 확인할 수 있다. vocab에 없는 단어를 wordpieces를 통해 생성해 낸다.</p> <p>아직 LSTM과 CNN에 대한 이해가 부족해서 이해하지 못할 얘기가 줄줄이 나오는데, 결론적으로는 이러한 character Based Model에 LSTM + Highway Network를 적용하면 꽤 풍부한 semantic and structural information을 추출해낼 수 있음을 확인하고 있다.(Character-Aware Neural Language Models, Yoon Kim, Yacine Jenite ... 2015)</p> <h2 id="hybrid-nmt"><a href="#hybrid-nmt" class="header-anchor">#</a> Hybrid NMT</h2> <p>필요할 때만 character level로 내려가는 모델.
seq2seq model을 기본으로 <unk>를 마주쳤을 때, char-level로 내려가서 decoding을 이어간다.</unk></p> <h2 id="highway-network-vs-lstm"><a href="#highway-network-vs-lstm" class="header-anchor">#</a> Highway network vs. LSTM</h2> <p><strong>Character Based model</strong> 에 적용할 수 있는 모델들. LSTM은 character based model로 word representation까지도 가능하게 해준다.</p> <p>Transformation을 original information을 처리하는 과정에서 적용한다. &quot;Functions askin to an LSTM cell&quot;</p> <p>LSTM 만 적용했을 때는 morphological 수준의 embedding에 가까운데(hard-rich-richer-richter) Highway 까지 적용하면 (eduard-gerard-edward-carl)</p> <h2 id="fasttest"><a href="#fasttest" class="header-anchor">#</a> FastTest, &lt;,&gt;</h2> <p>sort of next word embedding. Word2vec의 저자인 Mikolov 가 새롭게 들고 나온 모델로 skip-gram model에 character n-gram 을 적용하였다.</p> <p>where = &lt;wh, whe, her, ere, re&gt;, <where> 과 같이 6개 n-gram로 표현하고, 이 6개의 벡터를 모두 사용해서 context score을 구하고 이들을 모두 더함으로써 유사도를 구한다.</where></p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">5/8/2020, 8:58:40 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/TIL/CPP/pointer_examples.html" class="prev">
        Variouse Pointer examples
      </a></span> <span class="next"><a href="/TIL/ML/cssn224_2.html">
        CS224n 6강
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"><!----></div></div>
    <script src="/TIL/assets/js/app.2722f0f6.js" defer></script><script src="/TIL/assets/js/2.2c4c9c51.js" defer></script><script src="/TIL/assets/js/22.65a2edf5.js" defer></script>
  </body>
</html>
